{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating_sonnets_using_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "jdm0W0yol52F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> LSTM to generate Shakespearean Sonnet</h1>\n",
        "\n",
        "This implemetation of LSTM is based on what I learnt from Udacity Deep Learning with PyTorch course. While in the course we generated text from Anna Karenina, here I have leveraged LSTM to generate sonnet based on Shakespeare's style. To achieve this we will train the LSTM model with Shakespeare's Sonnets, the model will eventually learn his style and will be able to predict the next character of the text with current character as input to the it. "
      ]
    },
    {
      "metadata": {
        "id": "PIvG0TEZNjMa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#importing required libraries \n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_-9B4p_0pV2j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Pre-processing Data</h2>\n",
        "\n",
        "The file, <i>sonnets.txt</i>, contains all of 154 sonnets composed by Shakespeare. There are roughly 94k characters in the file for us to work with."
      ]
    },
    {
      "metadata": {
        "id": "fOJNQ-UZQiPe",
        "colab_type": "code",
        "outputId": "abeda4c4-ac6c-4ca9-871c-4617ab374d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#importing data\n",
        "with open(\"sonnets.txt\",\"r\") as sf:\n",
        "  data = sf.read()\n",
        "len(data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93775"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "1oHrqlxsUe8d",
        "colab_type": "code",
        "outputId": "06bff68c-b1eb-44a5-fae0-d372c7a72455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "#checking data\n",
        "print(data[:200])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From fairest creatures we desire increase,\n",
            "That thereby beautyâ€™s Rose might never die,\n",
            "But as the riper should by time decease,\n",
            "His tender heir might bear his memory:\n",
            "But thou, contracted to thine own\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ohDxzoLOUt0Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#character dictionary\n",
        "chars = tuple(set(data))\n",
        "int_2_char = dict(enumerate(chars))\n",
        "char_2_int = {ch:ii for ii,ch in int_2_char.items()}\n",
        "\n",
        "#encoded characters\n",
        "encoded = np.array([char_2_int[ch] for ch in data])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FGaTu8-tZp8d",
        "colab_type": "code",
        "outputId": "2e7c8a87-4310-4ae5-bc7f-65494110bb58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "encoded[:100]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([24,  8, 33, 27, 14, 47, 43, 61,  8, 13, 57, 32, 14, 19,  8, 13, 43,\n",
              "       32,  9,  8, 13, 57, 14, 22, 13, 14, 23, 13, 57, 61,  8, 13, 14, 61,\n",
              "       28, 19,  8, 13, 43, 57, 13, 40,  1, 37,  3, 43, 32, 14, 32,  3, 13,\n",
              "        8, 13, 62, 15, 14, 62, 13, 43,  9, 32, 15, 49, 57, 14, 50, 33, 57,\n",
              "       13, 14, 27, 61, 41,  3, 32, 14, 28, 13, 38, 13,  8, 14, 23, 61, 13,\n",
              "       40,  1,  4,  9, 32, 14, 43, 57, 14, 32,  3, 13, 14,  8, 61])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "IAXz3BP2aZtM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(array, n_labels):\n",
        "  \"\"\"method to one-hot encode characters  \n",
        "  array: np.array to be encoded\n",
        "  n_labels: no. of labels, size of dictionary\"\"\"\n",
        "  \n",
        "  #initialize encoded array \n",
        "  one_hot = np.zeros((np.multiply(*array.shape),n_labels),dtype = np.float32)\n",
        "  \n",
        "  #setting appropriate elements with ones\n",
        "  one_hot[np.arange(one_hot.shape[0]),array.flatten()] = 1\n",
        "  \n",
        "  #reshape to required shape\n",
        "  one_hot = one_hot.reshape((*array.shape,n_labels))\n",
        "  \n",
        "  return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DZung0gBfchf",
        "colab_type": "code",
        "outputId": "ae87329f-ec99-45bd-e147-bd19229e8a11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "#checking the method\n",
        "one_hot_encoding(np.array([[3,5,6]]),8)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "sznh4IU3wGvK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model is trained using batches of the data. The following method generates a pair of array, <i>x</i>  and <i>y</i>. Where <i>x</i> is array of required characters and <i>y</i> is array of characters next to that corresponding to <i>x</i>. While sequences in <i>x</i> are used as features, sequences in <i>y</i> are used as labels for that of <i>x</i>. "
      ]
    },
    {
      "metadata": {
        "id": "hWXjQFOigY9F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(array, batch_size, seq_len):\n",
        "  \"\"\"method to generate batches of data\n",
        "  array: one-hot encoded data as np.array\n",
        "  batch_size: no. of sequences required in each batch\n",
        "  seq_len: no. of characters in each sequence\"\"\"\n",
        "\n",
        "  #calculating characters in each batch\n",
        "  batch_size_total = batch_size * seq_len\n",
        "  \n",
        "  #no. of batches that can be genrated from the data\n",
        "  n_batches = len(array) // batch_size_total\n",
        "  \n",
        "  array = array[:n_batches * batch_size_total]\n",
        "  \n",
        "  array = array.reshape((batch_size, -1))\n",
        "  \n",
        "  for n in range(0, array.shape[1], seq_len):\n",
        "    \n",
        "    x = array[:, n : n + seq_len]#batch of required characters as features\n",
        "    y = np.zeros_like(x)#batch of coresponding x+1 character as labels\n",
        "    \n",
        "    try:\n",
        "      y[:,:-1], y[:,-1] = x[:,1:], array[:,n+seq_len]\n",
        "      \n",
        "    except IndexError:\n",
        "      y[:,:-1], y[:,-1] = x[:,1:], array[:,0]\n",
        "      \n",
        "    yield x,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q16_L3aDnDqy",
        "colab_type": "code",
        "outputId": "e124fc4c-2a76-43d2-9fda-158c5defdfd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "cell_type": "code",
      "source": [
        "#testing get_batches()\n",
        "for x,y in get_batches(encoded[:50],5,5):\n",
        "  print(\"x =\\n{}\".format(x))\n",
        "  print(\"Y =\\n{}\".format(y))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x =\n",
            "[[24  8 33 27 14]\n",
            " [57 32 14 19  8]\n",
            " [13 57 14 22 13]\n",
            " [ 8 13 14 61 28]\n",
            " [13 40  1 37  3]]\n",
            "Y =\n",
            "[[ 8 33 27 14 47]\n",
            " [32 14 19  8 13]\n",
            " [57 14 22 13 14]\n",
            " [13 14 61 28 19]\n",
            " [40  1 37  3 43]]\n",
            "x =\n",
            "[[47 43 61  8 13]\n",
            " [13 43 32  9  8]\n",
            " [14 23 13 57 61]\n",
            " [19  8 13 43 57]\n",
            " [43 32 14 32  3]]\n",
            "Y =\n",
            "[[43 61  8 13 24]\n",
            " [43 32  9  8 57]\n",
            " [23 13 57 61 13]\n",
            " [ 8 13 43 57  8]\n",
            " [32 14 32  3 13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "De8p3Md8yv0R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Building the model</h2>"
      ]
    },
    {
      "metadata": {
        "id": "rJrlQWvPnHOy",
        "colab_type": "code",
        "outputId": "182d684c-8837-4920-9cea-b474e70b83ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#testing for gpu availibility\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "device"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "hYlbiskMsD3W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharLSTM(nn.Module):\n",
        "  #class for the model\n",
        "  \n",
        "  def __init__(self, tokens_len, n_hidden=256, n_layers=2, drop_prob=0.5):\n",
        "    \n",
        "    super().__init__()\n",
        "    \n",
        "    self.chars_len = tokens_len#length of dictionary\n",
        "    self.n_hidden = n_hidden#no. of hidden layers \n",
        "    self.n_layers = n_layers#no. of layers of LSTM cell\n",
        "    self.drop_prob = drop_prob#dropout probability for regularization\n",
        "    \n",
        "    #LSTM layer of the model\n",
        "    self.lstm = nn.LSTM(self.chars_len, self.n_hidden, self.n_layers, dropout=self.drop_prob, batch_first=True)\n",
        "    \n",
        "    #Dropout layer of the model\n",
        "    self.dropout = nn.Dropout(self.drop_prob)\n",
        "    \n",
        "    #Fully connected layer of the model\n",
        "    self.fc = nn.Linear(self.n_hidden, self.chars_len)\n",
        "    \n",
        "    \n",
        "  def forward(self, x, hidden):\n",
        "    \n",
        "    output_r, hidden_out = self.lstm(x, hidden)\n",
        "    \n",
        "    out = self.dropout(output_r)\n",
        "    \n",
        "    #contiguos is used to reshape output of stacked LSTM cells\n",
        "    out = out.contiguous().view(-1,self.n_hidden)\n",
        "    \n",
        "    out = self.fc(out)\n",
        "    \n",
        "    return out, hidden_out\n",
        "  \n",
        "  \n",
        "  def init_hidden(self, batch_size, device):\n",
        "    #method to initialize hidden layer for the model\n",
        "    \n",
        "    weights = next(self.parameters()).data\n",
        "    \n",
        "    hidden = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "    \n",
        "    return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fLnW_MyW04gl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Training the model</h2>"
      ]
    },
    {
      "metadata": {
        "id": "aiqS-2Y49qG_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, data, epoch=20, batch_size=10, seq_len=50, lr=0.001, clip=5, val_frac=0.2, print_every=10):\n",
        "  \"\"\"train method is used to train the model \n",
        "  \n",
        "  model: CharLSTM object to be trained \n",
        "  data: data to be used to train the model. Data should be converted to int\n",
        "  batch_size: no. of sequences in each batch\n",
        "  seq_len: no. of characters in each sequence\n",
        "  lr: learning rate used by the optimizer to train the model\n",
        "  clip: clip the gradient for lstm to avoid exploding gradient\n",
        "  val_frac: fraction of data to be used for validation\n",
        "  print_every: number of steps to print training and validation loss\"\"\"\n",
        "  \n",
        "  #explicitly moving model to training mode\n",
        "  model.train()\n",
        "  \n",
        "  #initialising optimizer for the model\n",
        "  optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  #intialising loss function for the model\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  \n",
        "  #splitting validation and training data\n",
        "  val_idx = int(len(data) * (1 - val_frac))\n",
        "  data, val_data = data[:val_idx], data[val_idx:]\n",
        "  \n",
        "  model = model.to(device)\n",
        "  \n",
        "  counter = 0\n",
        "  n_chars = model.chars_len\n",
        "  train_losses = []\n",
        "  valid_losses = []\n",
        "  \n",
        "  \n",
        "  for e in range(epoch):\n",
        "      \n",
        "    #initialising hidden layers\n",
        "    h = model.init_hidden(batch_size, device)\n",
        "    \n",
        "    t_loss = []\n",
        "    \n",
        "    #for every batch\n",
        "    for x,y in get_batches(data, batch_size, seq_len):\n",
        "      \n",
        "      counter += 1\n",
        "      \n",
        "      \n",
        "      x = one_hot_encoding(x, n_chars)\n",
        "      inputs, targets = torch.from_numpy(x).to(device), torch.from_numpy(y).to(device)\n",
        "      \n",
        "      h = tuple([each.data for each in h])\n",
        "      \n",
        "      #clearing gradients on model parameters, if any\n",
        "      model.zero_grad()\n",
        "      \n",
        "      #feedforward \n",
        "      output, h = model(inputs, h)\n",
        "          \n",
        "      #calculationg loss\n",
        "      loss = criterion(output, targets.view(batch_size * seq_len))\n",
        "      \n",
        "      #backpropogation\n",
        "      loss.backward()\n",
        "      \n",
        "      #clearing extra gradient\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "      \n",
        "      #optimiser step\n",
        "      optim.step()\n",
        "      \n",
        "      t_loss.append(loss.item())\n",
        "      \n",
        "      #checking validation loss\n",
        "      if counter % print_every == 0:\n",
        "        \n",
        "        val_h = model.init_hidden(batch_size, device)\n",
        "        \n",
        "        val_losses = []\n",
        "        \n",
        "        #explicitly moving model to evaluation mode\n",
        "        model.eval()\n",
        "        \n",
        "        for x,y in get_batches(val_data, batch_size, seq_len):\n",
        "          \n",
        "          x = one_hot_encoding(x, n_chars)\n",
        "          \n",
        "          inputs, targets = torch.from_numpy(x).to(device), torch.from_numpy(y).to(device)\n",
        "          \n",
        "          val_h = tuple([each.data for each in h])\n",
        "          \n",
        "          output, h = model(inputs, val_h)\n",
        "          \n",
        "          val_loss = criterion(output, targets.view(batch_size * seq_len))\n",
        "          \n",
        "          val_losses.append(val_loss.item())\n",
        "        \n",
        "        model.train()\n",
        "        \n",
        "        \n",
        "        train_losses.append(np.mean(t_loss))\n",
        "        valid_losses.append(np.mean(val_losses))\n",
        "        \n",
        "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(train_losses[-1]),\n",
        "                      \"Val Loss: {:.4f}\".format(valid_losses[-1]))\n",
        "        \n",
        "  return train_losses, valid_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PtqJYjxtNfK-",
        "colab_type": "code",
        "outputId": "47e67a3e-2ea2-467e-ad26-2c4198e5f020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "#hyperparameters for initialising the model\n",
        "n_hidden = 128\n",
        "n_layers = 2\n",
        "drop_prob = 0.50\n",
        "\n",
        "#initialising the model\n",
        "model = CharLSTM(len(chars), n_hidden=n_hidden, n_layers=n_layers, drop_prob=drop_prob)\n",
        "model"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharLSTM(\n",
              "  (lstm): LSTM(63, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5)\n",
              "  (fc): Linear(in_features=128, out_features=63, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "CrnkuJmAKzxT",
        "colab_type": "code",
        "outputId": "afa73b1e-c02d-4ce7-8e9e-9b20d88d9532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1547
        }
      },
      "cell_type": "code",
      "source": [
        "#hyperparameters for training the model\n",
        "epochs = 60\n",
        "lr = 0.001\n",
        "batch_size = 20\n",
        "seq_len = 100\n",
        "print_every = 50\n",
        "\n",
        "#initiating training \n",
        "train_losses, val_losses = train(model, encoded, epoch=epochs, batch_size=batch_size, lr=lr, print_every = print_every)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/60... Step: 50... Loss: 3.3918... Val Loss: 3.1193\n",
            "Epoch: 2/60... Step: 100... Loss: 3.1481... Val Loss: 3.1139\n",
            "Epoch: 2/60... Step: 150... Loss: 3.1326... Val Loss: 3.1025\n",
            "Epoch: 3/60... Step: 200... Loss: 3.0974... Val Loss: 3.0459\n",
            "Epoch: 4/60... Step: 250... Loss: 2.9298... Val Loss: 2.8358\n",
            "Epoch: 4/60... Step: 300... Loss: 2.8383... Val Loss: 2.6810\n",
            "Epoch: 5/60... Step: 350... Loss: 2.6831... Val Loss: 2.5459\n",
            "Epoch: 6/60... Step: 400... Loss: 2.5612... Val Loss: 2.4585\n",
            "Epoch: 6/60... Step: 450... Loss: 2.5191... Val Loss: 2.3949\n",
            "Epoch: 7/60... Step: 500... Loss: 2.4591... Val Loss: 2.3559\n",
            "Epoch: 8/60... Step: 550... Loss: 2.4317... Val Loss: 2.3286\n",
            "Epoch: 8/60... Step: 600... Loss: 2.4069... Val Loss: 2.3102\n",
            "Epoch: 9/60... Step: 650... Loss: 2.3751... Val Loss: 2.2864\n",
            "Epoch: 10/60... Step: 700... Loss: 2.3543... Val Loss: 2.2675\n",
            "Epoch: 10/60... Step: 750... Loss: 2.3398... Val Loss: 2.2558\n",
            "Epoch: 11/60... Step: 800... Loss: 2.3215... Val Loss: 2.2388\n",
            "Epoch: 12/60... Step: 850... Loss: 2.3017... Val Loss: 2.2178\n",
            "Epoch: 12/60... Step: 900... Loss: 2.2847... Val Loss: 2.2030\n",
            "Epoch: 13/60... Step: 950... Loss: 2.2680... Val Loss: 2.1868\n",
            "Epoch: 14/60... Step: 1000... Loss: 2.2539... Val Loss: 2.1660\n",
            "Epoch: 14/60... Step: 1050... Loss: 2.2374... Val Loss: 2.1547\n",
            "Epoch: 15/60... Step: 1100... Loss: 2.2194... Val Loss: 2.1409\n",
            "Epoch: 16/60... Step: 1150... Loss: 2.2079... Val Loss: 2.1184\n",
            "Epoch: 16/60... Step: 1200... Loss: 2.1939... Val Loss: 2.1122\n",
            "Epoch: 17/60... Step: 1250... Loss: 2.1757... Val Loss: 2.0906\n",
            "Epoch: 18/60... Step: 1300... Loss: 2.1579... Val Loss: 2.0745\n",
            "Epoch: 18/60... Step: 1350... Loss: 2.1434... Val Loss: 2.0583\n",
            "Epoch: 19/60... Step: 1400... Loss: 2.1244... Val Loss: 2.0399\n",
            "Epoch: 20/60... Step: 1450... Loss: 2.1136... Val Loss: 2.0223\n",
            "Epoch: 20/60... Step: 1500... Loss: 2.0959... Val Loss: 2.0148\n",
            "Epoch: 21/60... Step: 1550... Loss: 2.0853... Val Loss: 2.0000\n",
            "Epoch: 22/60... Step: 1600... Loss: 2.0806... Val Loss: 1.9961\n",
            "Epoch: 22/60... Step: 1650... Loss: 2.0617... Val Loss: 1.9780\n",
            "Epoch: 23/60... Step: 1700... Loss: 2.0470... Val Loss: 1.9738\n",
            "Epoch: 24/60... Step: 1750... Loss: 2.0465... Val Loss: 1.9585\n",
            "Epoch: 24/60... Step: 1800... Loss: 2.0339... Val Loss: 1.9559\n",
            "Epoch: 25/60... Step: 1850... Loss: 2.0183... Val Loss: 1.9429\n",
            "Epoch: 26/60... Step: 1900... Loss: 2.0061... Val Loss: 1.9326\n",
            "Epoch: 26/60... Step: 1950... Loss: 1.9986... Val Loss: 1.9287\n",
            "Epoch: 27/60... Step: 2000... Loss: 1.9914... Val Loss: 1.9223\n",
            "Epoch: 28/60... Step: 2050... Loss: 1.9903... Val Loss: 1.9220\n",
            "Epoch: 28/60... Step: 2100... Loss: 1.9757... Val Loss: 1.9095\n",
            "Epoch: 29/60... Step: 2150... Loss: 1.9612... Val Loss: 1.8997\n",
            "Epoch: 30/60... Step: 2200... Loss: 1.9641... Val Loss: 1.8987\n",
            "Epoch: 30/60... Step: 2250... Loss: 1.9518... Val Loss: 1.8869\n",
            "Epoch: 31/60... Step: 2300... Loss: 1.9444... Val Loss: 1.8891\n",
            "Epoch: 32/60... Step: 2350... Loss: 1.9416... Val Loss: 1.8783\n",
            "Epoch: 32/60... Step: 2400... Loss: 1.9344... Val Loss: 1.8722\n",
            "Epoch: 33/60... Step: 2450... Loss: 1.9202... Val Loss: 1.8668\n",
            "Epoch: 34/60... Step: 2500... Loss: 1.9224... Val Loss: 1.8661\n",
            "Epoch: 34/60... Step: 2550... Loss: 1.9138... Val Loss: 1.8608\n",
            "Epoch: 35/60... Step: 2600... Loss: 1.9067... Val Loss: 1.8747\n",
            "Epoch: 36/60... Step: 2650... Loss: 1.9013... Val Loss: 1.8444\n",
            "Epoch: 36/60... Step: 2700... Loss: 1.8912... Val Loss: 1.8451\n",
            "Epoch: 37/60... Step: 2750... Loss: 1.8822... Val Loss: 1.8444\n",
            "Epoch: 38/60... Step: 2800... Loss: 1.8830... Val Loss: 1.8336\n",
            "Epoch: 38/60... Step: 2850... Loss: 1.8752... Val Loss: 1.8399\n",
            "Epoch: 39/60... Step: 2900... Loss: 1.8696... Val Loss: 1.8395\n",
            "Epoch: 40/60... Step: 2950... Loss: 1.8782... Val Loss: 1.8207\n",
            "Epoch: 40/60... Step: 3000... Loss: 1.8650... Val Loss: 1.8223\n",
            "Epoch: 41/60... Step: 3050... Loss: 1.8485... Val Loss: 1.8214\n",
            "Epoch: 42/60... Step: 3100... Loss: 1.8552... Val Loss: 1.8166\n",
            "Epoch: 42/60... Step: 3150... Loss: 1.8457... Val Loss: 1.8146\n",
            "Epoch: 43/60... Step: 3200... Loss: 1.8394... Val Loss: 1.8140\n",
            "Epoch: 44/60... Step: 3250... Loss: 1.8352... Val Loss: 1.8042\n",
            "Epoch: 44/60... Step: 3300... Loss: 1.8260... Val Loss: 1.8076\n",
            "Epoch: 45/60... Step: 3350... Loss: 1.8256... Val Loss: 1.8131\n",
            "Epoch: 46/60... Step: 3400... Loss: 1.8239... Val Loss: 1.7944\n",
            "Epoch: 46/60... Step: 3450... Loss: 1.8171... Val Loss: 1.7969\n",
            "Epoch: 47/60... Step: 3500... Loss: 1.8092... Val Loss: 1.7991\n",
            "Epoch: 48/60... Step: 3550... Loss: 1.8152... Val Loss: 1.7870\n",
            "Epoch: 48/60... Step: 3600... Loss: 1.8021... Val Loss: 1.7928\n",
            "Epoch: 49/60... Step: 3650... Loss: 1.7968... Val Loss: 1.7906\n",
            "Epoch: 50/60... Step: 3700... Loss: 1.8005... Val Loss: 1.7807\n",
            "Epoch: 50/60... Step: 3750... Loss: 1.7885... Val Loss: 1.7884\n",
            "Epoch: 51/60... Step: 3800... Loss: 1.7894... Val Loss: 1.7867\n",
            "Epoch: 52/60... Step: 3850... Loss: 1.7839... Val Loss: 1.7744\n",
            "Epoch: 52/60... Step: 3900... Loss: 1.7784... Val Loss: 1.7824\n",
            "Epoch: 53/60... Step: 3950... Loss: 1.7710... Val Loss: 1.7845\n",
            "Epoch: 54/60... Step: 4000... Loss: 1.7801... Val Loss: 1.7723\n",
            "Epoch: 54/60... Step: 4050... Loss: 1.7672... Val Loss: 1.7785\n",
            "Epoch: 55/60... Step: 4100... Loss: 1.7639... Val Loss: 1.7790\n",
            "Epoch: 56/60... Step: 4150... Loss: 1.7590... Val Loss: 1.7665\n",
            "Epoch: 56/60... Step: 4200... Loss: 1.7524... Val Loss: 1.7745\n",
            "Epoch: 57/60... Step: 4250... Loss: 1.7457... Val Loss: 1.7796\n",
            "Epoch: 58/60... Step: 4300... Loss: 1.7574... Val Loss: 1.7657\n",
            "Epoch: 58/60... Step: 4350... Loss: 1.7494... Val Loss: 1.7682\n",
            "Epoch: 59/60... Step: 4400... Loss: 1.7374... Val Loss: 1.7802\n",
            "Epoch: 60/60... Step: 4450... Loss: 1.7432... Val Loss: 1.7606\n",
            "Epoch: 60/60... Step: 4500... Loss: 1.7328... Val Loss: 1.7656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ylMWIpPT2n08",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Analysing and Testing</h2>"
      ]
    },
    {
      "metadata": {
        "id": "27xTn1H5OgNz",
        "colab_type": "code",
        "outputId": "19d6e335-ccb8-4473-b36a-09b34cb25f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#analysising training and validation loss over the training time\n",
        "plt.plot(train_losses)\n",
        "plt.plot(val_losses)\n",
        "plt.legend([\"Training Loss\",\"Validation Loss\"])\n",
        "plt.show()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFKCAYAAAAnj5dkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhCgw/wOPWuJy7k1LmgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Unmk1mUByYf3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is clear from the graph that if the model is trained any further it will overfit the training data. Even after trying with all possible hyperparameters this was the best result. This means that 94k characters aren't sufficient to train our model. Introducing more characters, around 10mil, will allow us to increase the size of our hidden layer which will increase the accuracy of our model."
      ]
    },
    {
      "metadata": {
        "id": "08tVse778u5Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#saving the model\n",
        "model_name = 'lstm_55_epoch_128_h_50_dp.net'\n",
        "\n",
        "checkpoint = {'n_hidden': model.n_hidden,\n",
        "              'n_layers': model.n_layers,\n",
        "              'state_dict': model.state_dict(),\n",
        "              'chars_len': model.chars_len}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0e5oLejURYrx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def predict(model, char, h=None, top_k=None):\n",
        "  ''' method to predict next character in the sequence\n",
        "  \n",
        "  model: model to be used to predict character\n",
        "  char: int of current character, i.e input for the model\n",
        "  h: hidden layer \n",
        "  top_k: no. of top k possible characters\n",
        "  '''\n",
        "\n",
        "  # tensor inputs\n",
        "  x = np.array([[char_2_int[char]]])\n",
        "  x = one_hot_encoding(x, model.chars_len)\n",
        "  inputs = torch.from_numpy(x).to(device)\n",
        "\n",
        "  # detach hidden state from history\n",
        "  h = tuple([each.data for each in h])\n",
        "  # get the output of the model\n",
        "  out, h = model(inputs, h)\n",
        "\n",
        "  # get the character probabilities\n",
        "  p = F.softmax(out, dim=1).data\n",
        "\n",
        "  p = p.cpu() # move to cpu\n",
        "\n",
        "  # get top characters\n",
        "  if top_k is None:\n",
        "      top_ch = np.arange(len(model.chars))\n",
        "  else:\n",
        "      p, top_ch = p.topk(top_k)\n",
        "      top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "  # select the likely next character with some element of randomness\n",
        "  p = p.numpy().squeeze()\n",
        "  char = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return int_2_char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VssUyVmLoPvp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(model, size, prime='The', top_k=None):\n",
        "  \"\"\"method to sample or generate string fraom predicted characters\n",
        "  \n",
        "  model: model used to predict\n",
        "  size: number of characters to be predicted\n",
        "  prime: intialising sequence\n",
        "  top_k: no. of top k possible characters\"\"\"\n",
        "  \n",
        "  model = model.to(device)\n",
        "  model.eval() # eval mode\n",
        "\n",
        "  # First off, run through the prime characters\n",
        "  chars = [ch for ch in prime]\n",
        "  h = model.init_hidden(1,device)\n",
        "  for ch in prime:\n",
        "      char, h = predict(model, ch, h, top_k=top_k)\n",
        "\n",
        "  chars.append(char)\n",
        "\n",
        "  # Now pass in the previous character and get a new one\n",
        "  for ii in range(size):\n",
        "      char, h = predict(model, chars[-1], h, top_k=top_k)\n",
        "      chars.append(char)\n",
        "\n",
        "  return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F0WjoAZKoWmy",
        "colab_type": "code",
        "outputId": "0e6cfc99-471b-461f-b1bd-07f1ebe54b95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "#testing the model\n",
        "print(sample(model, 200, prime='You', top_k=2))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You that shall, to the see,\n",
            "The world to stall to the store and tome,\n",
            "Then thou the sheer than the worth thee to the well,\n",
            "To to shall to the will the worth thou thee,\n",
            "And then my love all store thou the \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i3o_TG6u2zT6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "While there are meaningfull words in the output, there is substancial lack of structure in the content. This can be improved if the model is trained for longer time on a bigger data. Andrew Karpathy mentions in his post that he trained his model for an hour on Shakespeare's dramas to achieve the result mentioned in his post. <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">Andrew Karpathy's post</a>"
      ]
    }
  ]
}