{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating_sonnets_using_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "jdm0W0yol52F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> LSTM to generate Shakespearean Sonnet</h1>\n",
        "\n",
        "This implemetation of LSTM is based on what I learnt from Udacity Deep Learning with PyTorch course. While in the course we generated text from Anna Karenina, here I have leveraged LSTM to generate sonnet based on Shakespeare's style. To achieve this we will train the LSTM model with Shakespeare's Sonnets, the model will eventually learn his style and will be able to predict the next character of the text with current character as input to the it. "
      ]
    },
    {
      "metadata": {
        "id": "PIvG0TEZNjMa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#importing required libraries \n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_-9B4p_0pV2j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Pre-processing Data</h2>\n",
        "\n",
        "The file, <i>sonnets.txt</i>, contains all of 154 sonnets composed by Shakespeare. There are roughly 94k characters in the file for us to work with."
      ]
    },
    {
      "metadata": {
        "id": "fOJNQ-UZQiPe",
        "colab_type": "code",
        "outputId": "d27fa0de-04c6-4f3a-9742-25284cee8b49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#importing data\n",
        "with open(\"sonnets.txt\",\"r\") as sf:\n",
        "  data = sf.read()\n",
        "len(data)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93775"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "1oHrqlxsUe8d",
        "colab_type": "code",
        "outputId": "5c9b7029-e186-40f5-a3e1-dc7b23f38099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "#checking data\n",
        "print(data[:200])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From fairest creatures we desire increase,\n",
            "That thereby beautyâ€™s Rose might never die,\n",
            "But as the riper should by time decease,\n",
            "His tender heir might bear his memory:\n",
            "But thou, contracted to thine own\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ohDxzoLOUt0Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#character dictionary\n",
        "chars = tuple(set(data))\n",
        "int_2_char = dict(enumerate(chars))\n",
        "char_2_int = {ch:ii for ii,ch in int_2_char.items()}\n",
        "\n",
        "#encoded characters\n",
        "encoded = np.array([char_2_int[ch] for ch in data])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FGaTu8-tZp8d",
        "colab_type": "code",
        "outputId": "83f0ad9a-0c67-4cb8-86f1-a2d2b2f83317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "encoded[:100]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([43, 15, 40, 25, 24, 23, 16, 33, 15, 57, 62, 42, 24, 18, 15, 57, 16,\n",
              "       42,  1, 15, 57, 62, 24, 14, 57, 24,  3, 57, 62, 33, 15, 57, 24, 33,\n",
              "       28, 18, 15, 57, 16, 62, 57,  4, 11, 45, 35, 16, 42, 24, 42, 35, 57,\n",
              "       15, 57,  7, 55, 24,  7, 57, 16,  1, 42, 55, 19, 62, 24, 36, 40, 62,\n",
              "       57, 24, 25, 33,  8, 35, 42, 24, 28, 57, 31, 57, 15, 24,  3, 33, 57,\n",
              "        4, 11, 60,  1, 42, 24, 16, 62, 24, 42, 35, 57, 24, 15, 33])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "IAXz3BP2aZtM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(array, n_labels):\n",
        "  \"\"\"method to one-hot encode characters  \n",
        "  array: np.array to be encoded\n",
        "  n_labels: no. of labels, size of dictionary\"\"\"\n",
        "  \n",
        "  #initialize encoded array \n",
        "  one_hot = np.zeros((np.multiply(*array.shape),n_labels),dtype = np.float32)\n",
        "  \n",
        "  #setting appropriate elements with ones\n",
        "  one_hot[np.arange(one_hot.shape[0]),array.flatten()] = 1\n",
        "  \n",
        "  #reshape to required shape\n",
        "  one_hot = one_hot.reshape((*array.shape,n_labels))\n",
        "  \n",
        "  return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DZung0gBfchf",
        "colab_type": "code",
        "outputId": "a700f578-840a-4655-8b86-3debb7611a46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "#checking the method\n",
        "one_hot_encoding(np.array([[3,5,6]]),8)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "sznh4IU3wGvK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model is trained using batches of the data. The following method generates a pair of array, <i>x</i>  and <i>y</i>. Where <i>x</i> is array of required characters and <i>y</i> is array of characters next to that corresponding to <i>x</i>. While sequences in <i>x</i> are used as features, sequences in <i>y</i> are used as labels for that of <i>x</i>. "
      ]
    },
    {
      "metadata": {
        "id": "hWXjQFOigY9F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(array, batch_size, seq_len):\n",
        "  \"\"\"method to generate batches of data\n",
        "  array: one-hot encoded data as np.array\n",
        "  batch_size: no. of sequences required in each batch\n",
        "  seq_len: no. of characters in each sequence\"\"\"\n",
        "\n",
        "  #calculating characters in each batch\n",
        "  batch_size_total = batch_size * seq_len\n",
        "  \n",
        "  #no. of batches that can be genrated from the data\n",
        "  n_batches = len(array) // batch_size_total\n",
        "  \n",
        "  array = array[:n_batches * batch_size_total]\n",
        "  \n",
        "  array = array.reshape((batch_size, -1))\n",
        "  \n",
        "  for n in range(0, array.shape[1], seq_len):\n",
        "    \n",
        "    x = array[:, n : n + seq_len]#batch of required characters as features\n",
        "    y = np.zeros_like(x)#batch of coresponding x+1 character as labels\n",
        "    \n",
        "    try:\n",
        "      y[:,:-1], y[:,-1] = x[:,1:], array[:,n+seq_len]\n",
        "      \n",
        "    except IndexError:\n",
        "      y[:,:-1], y[:,-1] = x[:,1:], array[:,0]\n",
        "      \n",
        "    yield x,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q16_L3aDnDqy",
        "colab_type": "code",
        "outputId": "b1679ede-3288-46cc-970e-d57e0ad8a3e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "cell_type": "code",
      "source": [
        "#testing get_batches()\n",
        "for x,y in get_batches(encoded[:50],5,5):\n",
        "  print(\"x =\\n{}\".format(x))\n",
        "  print(\"Y =\\n{}\".format(y))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x =\n",
            "[[43 15 40 25 24]\n",
            " [62 42 24 18 15]\n",
            " [57 62 24 14 57]\n",
            " [15 57 24 33 28]\n",
            " [57  4 11 45 35]]\n",
            "Y =\n",
            "[[15 40 25 24 23]\n",
            " [42 24 18 15 57]\n",
            " [62 24 14 57 24]\n",
            " [57 24 33 28 18]\n",
            " [ 4 11 45 35 16]]\n",
            "x =\n",
            "[[23 16 33 15 57]\n",
            " [57 16 42  1 15]\n",
            " [24  3 57 62 33]\n",
            " [18 15 57 16 62]\n",
            " [16 42 24 42 35]]\n",
            "Y =\n",
            "[[16 33 15 57 43]\n",
            " [16 42  1 15 62]\n",
            " [ 3 57 62 33 57]\n",
            " [15 57 16 62 15]\n",
            " [42 24 42 35 57]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "De8p3Md8yv0R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Building the model</h2>"
      ]
    },
    {
      "metadata": {
        "id": "rJrlQWvPnHOy",
        "colab_type": "code",
        "outputId": "e3e26078-dad7-4521-8178-170bd1ddf8b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#testing for gpu availibility\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "device"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "hYlbiskMsD3W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharLSTM(nn.Module):\n",
        "  #class for the model\n",
        "  \n",
        "  def __init__(self, tokens_len, n_hidden=256, n_layers=2, drop_prob=0.5):\n",
        "    \n",
        "    super().__init__()\n",
        "    \n",
        "    self.chars_len = tokens_len#length of dictionary\n",
        "    self.n_hidden = n_hidden#no. of hidden layers \n",
        "    self.n_layers = n_layers#no. of layers of LSTM cell\n",
        "    self.drop_prob = drop_prob#dropout probability for regularization\n",
        "    \n",
        "    #LSTM layer of the model\n",
        "    self.lstm = nn.LSTM(self.chars_len, self.n_hidden, self.n_layers, dropout=self.drop_prob, batch_first=True)\n",
        "    \n",
        "    #Dropout layer of the model\n",
        "    self.dropout = nn.Dropout(self.drop_prob)\n",
        "    \n",
        "    #Fully connected layer of the model\n",
        "    self.fc = nn.Linear(self.n_hidden, self.chars_len)\n",
        "    \n",
        "    \n",
        "  def forward(self, x, hidden):\n",
        "    \n",
        "    output_r, hidden_out = self.lstm(x, hidden)\n",
        "    \n",
        "    out = self.dropout(output_r)\n",
        "    \n",
        "    #contiguos is used to reshape output of stacked LSTM cells\n",
        "    out = out.contiguous().view(-1,self.n_hidden)\n",
        "    \n",
        "    out = self.fc(out)\n",
        "    \n",
        "    return out, hidden_out\n",
        "  \n",
        "  \n",
        "  def init_hidden(self, batch_size, device):\n",
        "    #method to initialize hidden layer for the model\n",
        "    \n",
        "    weights = next(self.parameters()).data\n",
        "    \n",
        "    hidden = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "    \n",
        "    return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fLnW_MyW04gl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Training the model</h2>"
      ]
    },
    {
      "metadata": {
        "id": "aiqS-2Y49qG_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, data, epoch=20, batch_size=10, seq_len=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "  \"\"\"train method is used to train the model \n",
        "  \n",
        "  model: CharLSTM object to be trained \n",
        "  data: data to be used to train the model. Data should be converted to int\n",
        "  batch_size: no. of sequences in each batch\n",
        "  seq_len: no. of characters in each sequence\n",
        "  lr: learning rate used by the optimizer to train the model\n",
        "  clip: clip the gradient for lstm to avoid exploding gradient\n",
        "  val_frac: fraction of data to be used for validation\n",
        "  print_every: number of steps to print training and validation loss\"\"\"\n",
        "  \n",
        "  #explicitly moving model to training mode\n",
        "  model.train()\n",
        "  \n",
        "  #initialising optimizer for the model\n",
        "  optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  #intialising loss function for the model\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  \n",
        "  #splitting validation and training data\n",
        "  val_idx = int(len(data) * (1 - val_frac))\n",
        "  data, val_data = data[:val_idx], data[val_idx:]\n",
        "  \n",
        "  model = model.to(device)\n",
        "  \n",
        "  counter = 0\n",
        "  n_chars = model.chars_len\n",
        "  train_losses = []\n",
        "  valid_losses = []\n",
        "  \n",
        "  for e in range(epoch):\n",
        "    \n",
        "    #initialising hidden layers\n",
        "    h = model.init_hidden(batch_size, device)\n",
        "    \n",
        "    #for every batch\n",
        "    for x,y in get_batches(data, batch_size, seq_len):\n",
        "      \n",
        "      counter += 1\n",
        "      \n",
        "      \n",
        "      x = one_hot_encoding(x, n_chars)\n",
        "      inputs, targets = torch.from_numpy(x).to(device), torch.from_numpy(y).to(device)\n",
        "      \n",
        "      h = tuple([each.data for each in h])\n",
        "      \n",
        "      #clearing gradients on model parameters, if any\n",
        "      model.zero_grad()\n",
        "      \n",
        "      #feedforward \n",
        "      output, h = model(inputs, h)\n",
        "          \n",
        "      #calculationg loss\n",
        "      loss = criterion(output, targets.view(batch_size * seq_len))\n",
        "      \n",
        "      #backpropogation\n",
        "      loss.backward()\n",
        "      \n",
        "      #clearing extra gradient\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "      \n",
        "      #optimiser step\n",
        "      optim.step()\n",
        "      \n",
        "      #checking validation loss\n",
        "      if counter % print_every == 0:\n",
        "        \n",
        "        val_h = model.init_hidden(batch_size, device)\n",
        "        \n",
        "        val_losses = []\n",
        "        \n",
        "        #explicitly moving model to evaluation mode\n",
        "        model.eval()\n",
        "        \n",
        "        for x,y in get_batches(val_data, batch_size, seq_len):\n",
        "          \n",
        "          x = one_hot_encoding(x, n_chars)\n",
        "          \n",
        "          inputs, targets = torch.from_numpy(x).to(device), torch.from_numpy(y).to(device)\n",
        "          \n",
        "          val_h = tuple([each.data for each in h])\n",
        "          \n",
        "          output, h = model(inputs, val_h)\n",
        "          \n",
        "          val_loss = criterion(output, targets.view(batch_size * seq_len))\n",
        "          \n",
        "          val_losses.append(val_loss.item())\n",
        "        \n",
        "        model.train()\n",
        "        \n",
        "        train_losses.append(loss.item())\n",
        "        valid_losses.append(np.mean(val_losses))\n",
        "        \n",
        "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(train_losses[-1]),\n",
        "                      \"Val Loss: {:.4f}\".format(valid_losses[-1]))\n",
        "        \n",
        "  return train_losses, valid_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PtqJYjxtNfK-",
        "colab_type": "code",
        "outputId": "a0cb75e3-0360-49ab-c3c8-629aef09b3a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "#hyperparameters for initialising the model\n",
        "n_hidden = 128\n",
        "n_layers = 2\n",
        "drop_prob = 0.50\n",
        "\n",
        "#initialising the model\n",
        "model = CharLSTM(len(chars), n_hidden=n_hidden, n_layers=n_layers, drop_prob=drop_prob)\n",
        "model"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharLSTM(\n",
              "  (lstm): LSTM(63, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5)\n",
              "  (fc): Linear(in_features=128, out_features=63, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "CrnkuJmAKzxT",
        "colab_type": "code",
        "outputId": "0e602d86-bc4e-4c26-f08e-5f80c6caac5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7871
        }
      },
      "cell_type": "code",
      "source": [
        "#hyperparameters for training the model\n",
        "epochs = 55\n",
        "lr = 0.001\n",
        "batch_size = 20\n",
        "seq_len = 100\n",
        "\n",
        "#initiating training \n",
        "train_losses, val_losses = train(model, encoded, epoch=epochs, batch_size=batch_size, lr=lr)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/55... Step: 10... Loss: 3.8848... Val Loss: 3.7501\n",
            "Epoch: 1/55... Step: 20... Loss: 3.2171... Val Loss: 3.1688\n",
            "Epoch: 1/55... Step: 30... Loss: 3.1652... Val Loss: 3.1302\n",
            "Epoch: 1/55... Step: 40... Loss: 3.1836... Val Loss: 3.1282\n",
            "Epoch: 1/55... Step: 50... Loss: 3.1954... Val Loss: 3.1220\n",
            "Epoch: 1/55... Step: 60... Loss: 3.1452... Val Loss: 3.1205\n",
            "Epoch: 1/55... Step: 70... Loss: 3.1079... Val Loss: 3.1176\n",
            "Epoch: 1/55... Step: 80... Loss: 3.1709... Val Loss: 3.1194\n",
            "Epoch: 2/55... Step: 90... Loss: 3.1793... Val Loss: 3.1178\n",
            "Epoch: 2/55... Step: 100... Loss: 3.1551... Val Loss: 3.1172\n",
            "Epoch: 2/55... Step: 110... Loss: 3.0993... Val Loss: 3.1158\n",
            "Epoch: 2/55... Step: 120... Loss: 3.1990... Val Loss: 3.1175\n",
            "Epoch: 2/55... Step: 130... Loss: 3.1336... Val Loss: 3.1137\n",
            "Epoch: 2/55... Step: 140... Loss: 3.1093... Val Loss: 3.1127\n",
            "Epoch: 2/55... Step: 150... Loss: 3.1053... Val Loss: 3.1093\n",
            "Epoch: 2/55... Step: 160... Loss: 3.1718... Val Loss: 3.1082\n",
            "Epoch: 3/55... Step: 170... Loss: 3.1512... Val Loss: 3.1055\n",
            "Epoch: 3/55... Step: 180... Loss: 3.1270... Val Loss: 3.1004\n",
            "Epoch: 3/55... Step: 190... Loss: 3.0796... Val Loss: 3.0947\n",
            "Epoch: 3/55... Step: 200... Loss: 3.0883... Val Loss: 3.0864\n",
            "Epoch: 3/55... Step: 210... Loss: 3.0771... Val Loss: 3.0742\n",
            "Epoch: 3/55... Step: 220... Loss: 3.0810... Val Loss: 3.0597\n",
            "Epoch: 3/55... Step: 230... Loss: 3.0473... Val Loss: 3.0228\n",
            "Epoch: 3/55... Step: 240... Loss: 2.9995... Val Loss: 2.9782\n",
            "Epoch: 3/55... Step: 250... Loss: 2.9539... Val Loss: 2.9347\n",
            "Epoch: 4/55... Step: 260... Loss: 2.8974... Val Loss: 2.8909\n",
            "Epoch: 4/55... Step: 270... Loss: 2.8973... Val Loss: 2.8502\n",
            "Epoch: 4/55... Step: 280... Loss: 2.8744... Val Loss: 2.8246\n",
            "Epoch: 4/55... Step: 290... Loss: 2.8759... Val Loss: 2.8049\n",
            "Epoch: 4/55... Step: 300... Loss: 2.8453... Val Loss: 2.7543\n",
            "Epoch: 4/55... Step: 310... Loss: 2.8120... Val Loss: 2.7274\n",
            "Epoch: 4/55... Step: 320... Loss: 2.7817... Val Loss: 2.6982\n",
            "Epoch: 4/55... Step: 330... Loss: 2.7800... Val Loss: 2.6610\n",
            "Epoch: 5/55... Step: 340... Loss: 2.6574... Val Loss: 2.6324\n",
            "Epoch: 5/55... Step: 350... Loss: 2.6144... Val Loss: 2.6029\n",
            "Epoch: 5/55... Step: 360... Loss: 2.6197... Val Loss: 2.5760\n",
            "Epoch: 5/55... Step: 370... Loss: 2.6238... Val Loss: 2.5520\n",
            "Epoch: 5/55... Step: 380... Loss: 2.6174... Val Loss: 2.5289\n",
            "Epoch: 5/55... Step: 390... Loss: 2.5700... Val Loss: 2.5162\n",
            "Epoch: 5/55... Step: 400... Loss: 2.5863... Val Loss: 2.4943\n",
            "Epoch: 5/55... Step: 410... Loss: 2.5897... Val Loss: 2.4800\n",
            "Epoch: 5/55... Step: 420... Loss: 2.5758... Val Loss: 2.4628\n",
            "Epoch: 6/55... Step: 430... Loss: 2.5152... Val Loss: 2.4510\n",
            "Epoch: 6/55... Step: 440... Loss: 2.5379... Val Loss: 2.4291\n",
            "Epoch: 6/55... Step: 450... Loss: 2.4677... Val Loss: 2.4173\n",
            "Epoch: 6/55... Step: 460... Loss: 2.4601... Val Loss: 2.4050\n",
            "Epoch: 6/55... Step: 470... Loss: 2.5238... Val Loss: 2.4002\n",
            "Epoch: 6/55... Step: 480... Loss: 2.5065... Val Loss: 2.3962\n",
            "Epoch: 6/55... Step: 490... Loss: 2.4489... Val Loss: 2.3804\n",
            "Epoch: 6/55... Step: 500... Loss: 2.4737... Val Loss: 2.3697\n",
            "Epoch: 7/55... Step: 510... Loss: 2.4925... Val Loss: 2.3616\n",
            "Epoch: 7/55... Step: 520... Loss: 2.4555... Val Loss: 2.3536\n",
            "Epoch: 7/55... Step: 530... Loss: 2.3857... Val Loss: 2.3471\n",
            "Epoch: 7/55... Step: 540... Loss: 2.4759... Val Loss: 2.3462\n",
            "Epoch: 7/55... Step: 550... Loss: 2.4036... Val Loss: 2.3367\n",
            "Epoch: 7/55... Step: 560... Loss: 2.3914... Val Loss: 2.3321\n",
            "Epoch: 7/55... Step: 570... Loss: 2.3769... Val Loss: 2.3280\n",
            "Epoch: 7/55... Step: 580... Loss: 2.4821... Val Loss: 2.3226\n",
            "Epoch: 8/55... Step: 590... Loss: 2.4456... Val Loss: 2.3283\n",
            "Epoch: 8/55... Step: 600... Loss: 2.3848... Val Loss: 2.3167\n",
            "Epoch: 8/55... Step: 610... Loss: 2.3512... Val Loss: 2.3161\n",
            "Epoch: 8/55... Step: 620... Loss: 2.4644... Val Loss: 2.3092\n",
            "Epoch: 8/55... Step: 630... Loss: 2.3800... Val Loss: 2.3048\n",
            "Epoch: 8/55... Step: 640... Loss: 2.3897... Val Loss: 2.3106\n",
            "Epoch: 8/55... Step: 650... Loss: 2.3936... Val Loss: 2.2961\n",
            "Epoch: 8/55... Step: 660... Loss: 2.3475... Val Loss: 2.3008\n",
            "Epoch: 8/55... Step: 670... Loss: 2.3510... Val Loss: 2.3025\n",
            "Epoch: 9/55... Step: 680... Loss: 2.3492... Val Loss: 2.2994\n",
            "Epoch: 9/55... Step: 690... Loss: 2.3719... Val Loss: 2.2852\n",
            "Epoch: 9/55... Step: 700... Loss: 2.3344... Val Loss: 2.2809\n",
            "Epoch: 9/55... Step: 710... Loss: 2.4322... Val Loss: 2.2822\n",
            "Epoch: 9/55... Step: 720... Loss: 2.3542... Val Loss: 2.2742\n",
            "Epoch: 9/55... Step: 730... Loss: 2.3837... Val Loss: 2.2746\n",
            "Epoch: 9/55... Step: 740... Loss: 2.3688... Val Loss: 2.2718\n",
            "Epoch: 9/55... Step: 750... Loss: 2.3893... Val Loss: 2.2720\n",
            "Epoch: 10/55... Step: 760... Loss: 2.3412... Val Loss: 2.2779\n",
            "Epoch: 10/55... Step: 770... Loss: 2.2631... Val Loss: 2.2651\n",
            "Epoch: 10/55... Step: 780... Loss: 2.2602... Val Loss: 2.2602\n",
            "Epoch: 10/55... Step: 790... Loss: 2.3283... Val Loss: 2.2560\n",
            "Epoch: 10/55... Step: 800... Loss: 2.3237... Val Loss: 2.2522\n",
            "Epoch: 10/55... Step: 810... Loss: 2.3017... Val Loss: 2.2550\n",
            "Epoch: 10/55... Step: 820... Loss: 2.3607... Val Loss: 2.2506\n",
            "Epoch: 10/55... Step: 830... Loss: 2.3701... Val Loss: 2.2479\n",
            "Epoch: 10/55... Step: 840... Loss: 2.3403... Val Loss: 2.2487\n",
            "Epoch: 11/55... Step: 850... Loss: 2.2927... Val Loss: 2.2430\n",
            "Epoch: 11/55... Step: 860... Loss: 2.3711... Val Loss: 2.2363\n",
            "Epoch: 11/55... Step: 870... Loss: 2.2774... Val Loss: 2.2332\n",
            "Epoch: 11/55... Step: 880... Loss: 2.2838... Val Loss: 2.2309\n",
            "Epoch: 11/55... Step: 890... Loss: 2.3352... Val Loss: 2.2290\n",
            "Epoch: 11/55... Step: 900... Loss: 2.3277... Val Loss: 2.2288\n",
            "Epoch: 11/55... Step: 910... Loss: 2.2748... Val Loss: 2.2271\n",
            "Epoch: 11/55... Step: 920... Loss: 2.2863... Val Loss: 2.2233\n",
            "Epoch: 12/55... Step: 930... Loss: 2.3442... Val Loss: 2.2187\n",
            "Epoch: 12/55... Step: 940... Loss: 2.2915... Val Loss: 2.2175\n",
            "Epoch: 12/55... Step: 950... Loss: 2.2442... Val Loss: 2.2156\n",
            "Epoch: 12/55... Step: 960... Loss: 2.3302... Val Loss: 2.2122\n",
            "Epoch: 12/55... Step: 970... Loss: 2.2403... Val Loss: 2.2089\n",
            "Epoch: 12/55... Step: 980... Loss: 2.2825... Val Loss: 2.2074\n",
            "Epoch: 12/55... Step: 990... Loss: 2.2207... Val Loss: 2.2076\n",
            "Epoch: 12/55... Step: 1000... Loss: 2.3169... Val Loss: 2.2015\n",
            "Epoch: 13/55... Step: 1010... Loss: 2.3346... Val Loss: 2.2000\n",
            "Epoch: 13/55... Step: 1020... Loss: 2.2315... Val Loss: 2.1980\n",
            "Epoch: 13/55... Step: 1030... Loss: 2.2150... Val Loss: 2.1939\n",
            "Epoch: 13/55... Step: 1040... Loss: 2.2859... Val Loss: 2.1892\n",
            "Epoch: 13/55... Step: 1050... Loss: 2.2355... Val Loss: 2.1849\n",
            "Epoch: 13/55... Step: 1060... Loss: 2.2312... Val Loss: 2.1849\n",
            "Epoch: 13/55... Step: 1070... Loss: 2.2348... Val Loss: 2.1813\n",
            "Epoch: 13/55... Step: 1080... Loss: 2.2144... Val Loss: 2.1768\n",
            "Epoch: 13/55... Step: 1090... Loss: 2.2296... Val Loss: 2.1770\n",
            "Epoch: 14/55... Step: 1100... Loss: 2.2062... Val Loss: 2.1747\n",
            "Epoch: 14/55... Step: 1110... Loss: 2.2925... Val Loss: 2.1717\n",
            "Epoch: 14/55... Step: 1120... Loss: 2.2056... Val Loss: 2.1658\n",
            "Epoch: 14/55... Step: 1130... Loss: 2.2784... Val Loss: 2.1669\n",
            "Epoch: 14/55... Step: 1140... Loss: 2.2426... Val Loss: 2.1605\n",
            "Epoch: 14/55... Step: 1150... Loss: 2.2337... Val Loss: 2.1598\n",
            "Epoch: 14/55... Step: 1160... Loss: 2.2553... Val Loss: 2.1571\n",
            "Epoch: 14/55... Step: 1170... Loss: 2.2466... Val Loss: 2.1553\n",
            "Epoch: 15/55... Step: 1180... Loss: 2.1967... Val Loss: 2.1516\n",
            "Epoch: 15/55... Step: 1190... Loss: 2.1322... Val Loss: 2.1485\n",
            "Epoch: 15/55... Step: 1200... Loss: 2.1379... Val Loss: 2.1390\n",
            "Epoch: 15/55... Step: 1210... Loss: 2.1846... Val Loss: 2.1360\n",
            "Epoch: 15/55... Step: 1220... Loss: 2.1938... Val Loss: 2.1308\n",
            "Epoch: 15/55... Step: 1230... Loss: 2.1812... Val Loss: 2.1308\n",
            "Epoch: 15/55... Step: 1240... Loss: 2.2421... Val Loss: 2.1256\n",
            "Epoch: 15/55... Step: 1250... Loss: 2.2605... Val Loss: 2.1211\n",
            "Epoch: 15/55... Step: 1260... Loss: 2.1827... Val Loss: 2.1209\n",
            "Epoch: 16/55... Step: 1270... Loss: 2.1604... Val Loss: 2.1238\n",
            "Epoch: 16/55... Step: 1280... Loss: 2.2408... Val Loss: 2.1089\n",
            "Epoch: 16/55... Step: 1290... Loss: 2.1735... Val Loss: 2.1068\n",
            "Epoch: 16/55... Step: 1300... Loss: 2.1435... Val Loss: 2.1007\n",
            "Epoch: 16/55... Step: 1310... Loss: 2.2200... Val Loss: 2.0982\n",
            "Epoch: 16/55... Step: 1320... Loss: 2.1713... Val Loss: 2.0963\n",
            "Epoch: 16/55... Step: 1330... Loss: 2.1717... Val Loss: 2.0953\n",
            "Epoch: 16/55... Step: 1340... Loss: 2.1600... Val Loss: 2.0906\n",
            "Epoch: 17/55... Step: 1350... Loss: 2.1861... Val Loss: 2.0882\n",
            "Epoch: 17/55... Step: 1360... Loss: 2.1621... Val Loss: 2.0819\n",
            "Epoch: 17/55... Step: 1370... Loss: 2.1451... Val Loss: 2.0794\n",
            "Epoch: 17/55... Step: 1380... Loss: 2.1621... Val Loss: 2.0782\n",
            "Epoch: 17/55... Step: 1390... Loss: 2.1481... Val Loss: 2.0751\n",
            "Epoch: 17/55... Step: 1400... Loss: 2.1207... Val Loss: 2.0720\n",
            "Epoch: 17/55... Step: 1410... Loss: 2.0454... Val Loss: 2.0701\n",
            "Epoch: 17/55... Step: 1420... Loss: 2.1905... Val Loss: 2.0647\n",
            "Epoch: 18/55... Step: 1430... Loss: 2.1588... Val Loss: 2.0646\n",
            "Epoch: 18/55... Step: 1440... Loss: 2.0992... Val Loss: 2.0651\n",
            "Epoch: 18/55... Step: 1450... Loss: 2.0922... Val Loss: 2.0572\n",
            "Epoch: 18/55... Step: 1460... Loss: 2.1335... Val Loss: 2.0553\n",
            "Epoch: 18/55... Step: 1470... Loss: 2.0835... Val Loss: 2.0497\n",
            "Epoch: 18/55... Step: 1480... Loss: 2.1136... Val Loss: 2.0532\n",
            "Epoch: 18/55... Step: 1490... Loss: 2.1052... Val Loss: 2.0514\n",
            "Epoch: 18/55... Step: 1500... Loss: 2.0776... Val Loss: 2.0519\n",
            "Epoch: 18/55... Step: 1510... Loss: 2.0683... Val Loss: 2.0460\n",
            "Epoch: 19/55... Step: 1520... Loss: 2.1181... Val Loss: 2.0398\n",
            "Epoch: 19/55... Step: 1530... Loss: 2.1354... Val Loss: 2.0410\n",
            "Epoch: 19/55... Step: 1540... Loss: 2.0443... Val Loss: 2.0354\n",
            "Epoch: 19/55... Step: 1550... Loss: 2.1612... Val Loss: 2.0368\n",
            "Epoch: 19/55... Step: 1560... Loss: 2.1378... Val Loss: 2.0343\n",
            "Epoch: 19/55... Step: 1570... Loss: 2.1246... Val Loss: 2.0350\n",
            "Epoch: 19/55... Step: 1580... Loss: 2.1256... Val Loss: 2.0316\n",
            "Epoch: 19/55... Step: 1590... Loss: 2.1299... Val Loss: 2.0247\n",
            "Epoch: 20/55... Step: 1600... Loss: 2.0982... Val Loss: 2.0260\n",
            "Epoch: 20/55... Step: 1610... Loss: 2.0167... Val Loss: 2.0257\n",
            "Epoch: 20/55... Step: 1620... Loss: 2.0387... Val Loss: 2.0226\n",
            "Epoch: 20/55... Step: 1630... Loss: 2.0390... Val Loss: 2.0196\n",
            "Epoch: 20/55... Step: 1640... Loss: 2.0989... Val Loss: 2.0165\n",
            "Epoch: 20/55... Step: 1650... Loss: 2.0678... Val Loss: 2.0198\n",
            "Epoch: 20/55... Step: 1660... Loss: 2.1309... Val Loss: 2.0158\n",
            "Epoch: 20/55... Step: 1670... Loss: 2.1252... Val Loss: 2.0053\n",
            "Epoch: 20/55... Step: 1680... Loss: 2.0881... Val Loss: 2.0070\n",
            "Epoch: 21/55... Step: 1690... Loss: 2.0737... Val Loss: 2.0057\n",
            "Epoch: 21/55... Step: 1700... Loss: 2.0964... Val Loss: 2.0017\n",
            "Epoch: 21/55... Step: 1710... Loss: 2.0134... Val Loss: 1.9995\n",
            "Epoch: 21/55... Step: 1720... Loss: 2.0261... Val Loss: 1.9993\n",
            "Epoch: 21/55... Step: 1730... Loss: 2.0987... Val Loss: 1.9971\n",
            "Epoch: 21/55... Step: 1740... Loss: 2.0752... Val Loss: 1.9982\n",
            "Epoch: 21/55... Step: 1750... Loss: 2.0407... Val Loss: 2.0019\n",
            "Epoch: 21/55... Step: 1760... Loss: 2.0484... Val Loss: 1.9964\n",
            "Epoch: 22/55... Step: 1770... Loss: 2.0840... Val Loss: 1.9948\n",
            "Epoch: 22/55... Step: 1780... Loss: 2.0356... Val Loss: 1.9937\n",
            "Epoch: 22/55... Step: 1790... Loss: 2.0367... Val Loss: 1.9895\n",
            "Epoch: 22/55... Step: 1800... Loss: 2.0868... Val Loss: 1.9885\n",
            "Epoch: 22/55... Step: 1810... Loss: 2.0750... Val Loss: 1.9858\n",
            "Epoch: 22/55... Step: 1820... Loss: 2.0307... Val Loss: 1.9863\n",
            "Epoch: 22/55... Step: 1830... Loss: 1.9644... Val Loss: 1.9856\n",
            "Epoch: 22/55... Step: 1840... Loss: 2.0807... Val Loss: 1.9801\n",
            "Epoch: 23/55... Step: 1850... Loss: 2.1212... Val Loss: 1.9817\n",
            "Epoch: 23/55... Step: 1860... Loss: 2.0007... Val Loss: 1.9788\n",
            "Epoch: 23/55... Step: 1870... Loss: 2.0346... Val Loss: 1.9745\n",
            "Epoch: 23/55... Step: 1880... Loss: 2.0316... Val Loss: 1.9758\n",
            "Epoch: 23/55... Step: 1890... Loss: 1.9978... Val Loss: 1.9717\n",
            "Epoch: 23/55... Step: 1900... Loss: 1.9982... Val Loss: 1.9731\n",
            "Epoch: 23/55... Step: 1910... Loss: 1.9879... Val Loss: 1.9706\n",
            "Epoch: 23/55... Step: 1920... Loss: 1.9755... Val Loss: 1.9686\n",
            "Epoch: 23/55... Step: 1930... Loss: 1.9880... Val Loss: 1.9678\n",
            "Epoch: 24/55... Step: 1940... Loss: 2.0022... Val Loss: 1.9695\n",
            "Epoch: 24/55... Step: 1950... Loss: 2.0329... Val Loss: 1.9665\n",
            "Epoch: 24/55... Step: 1960... Loss: 1.9707... Val Loss: 1.9635\n",
            "Epoch: 24/55... Step: 1970... Loss: 2.0618... Val Loss: 1.9638\n",
            "Epoch: 24/55... Step: 1980... Loss: 2.0634... Val Loss: 1.9591\n",
            "Epoch: 24/55... Step: 1990... Loss: 2.0415... Val Loss: 1.9605\n",
            "Epoch: 24/55... Step: 2000... Loss: 2.0507... Val Loss: 1.9581\n",
            "Epoch: 24/55... Step: 2010... Loss: 2.0498... Val Loss: 1.9528\n",
            "Epoch: 25/55... Step: 2020... Loss: 2.0069... Val Loss: 1.9531\n",
            "Epoch: 25/55... Step: 2030... Loss: 1.9142... Val Loss: 1.9514\n",
            "Epoch: 25/55... Step: 2040... Loss: 1.9726... Val Loss: 1.9488\n",
            "Epoch: 25/55... Step: 2050... Loss: 1.9872... Val Loss: 1.9510\n",
            "Epoch: 25/55... Step: 2060... Loss: 2.0106... Val Loss: 1.9454\n",
            "Epoch: 25/55... Step: 2070... Loss: 2.0322... Val Loss: 1.9462\n",
            "Epoch: 25/55... Step: 2080... Loss: 2.0321... Val Loss: 1.9445\n",
            "Epoch: 25/55... Step: 2090... Loss: 2.0664... Val Loss: 1.9389\n",
            "Epoch: 25/55... Step: 2100... Loss: 2.0395... Val Loss: 1.9408\n",
            "Epoch: 26/55... Step: 2110... Loss: 2.0052... Val Loss: 1.9381\n",
            "Epoch: 26/55... Step: 2120... Loss: 2.0534... Val Loss: 1.9349\n",
            "Epoch: 26/55... Step: 2130... Loss: 1.9625... Val Loss: 1.9333\n",
            "Epoch: 26/55... Step: 2140... Loss: 1.9784... Val Loss: 1.9357\n",
            "Epoch: 26/55... Step: 2150... Loss: 2.0409... Val Loss: 1.9308\n",
            "Epoch: 26/55... Step: 2160... Loss: 1.9972... Val Loss: 1.9343\n",
            "Epoch: 26/55... Step: 2170... Loss: 1.9310... Val Loss: 1.9361\n",
            "Epoch: 26/55... Step: 2180... Loss: 1.9976... Val Loss: 1.9336\n",
            "Epoch: 27/55... Step: 2190... Loss: 2.0053... Val Loss: 1.9367\n",
            "Epoch: 27/55... Step: 2200... Loss: 1.9571... Val Loss: 1.9350\n",
            "Epoch: 27/55... Step: 2210... Loss: 1.9751... Val Loss: 1.9262\n",
            "Epoch: 27/55... Step: 2220... Loss: 2.0080... Val Loss: 1.9277\n",
            "Epoch: 27/55... Step: 2230... Loss: 1.9788... Val Loss: 1.9250\n",
            "Epoch: 27/55... Step: 2240... Loss: 1.9993... Val Loss: 1.9246\n",
            "Epoch: 27/55... Step: 2250... Loss: 1.8556... Val Loss: 1.9241\n",
            "Epoch: 27/55... Step: 2260... Loss: 2.0267... Val Loss: 1.9212\n",
            "Epoch: 28/55... Step: 2270... Loss: 2.0085... Val Loss: 1.9211\n",
            "Epoch: 28/55... Step: 2280... Loss: 1.9444... Val Loss: 1.9195\n",
            "Epoch: 28/55... Step: 2290... Loss: 1.9582... Val Loss: 1.9149\n",
            "Epoch: 28/55... Step: 2300... Loss: 1.9470... Val Loss: 1.9173\n",
            "Epoch: 28/55... Step: 2310... Loss: 1.9048... Val Loss: 1.9143\n",
            "Epoch: 28/55... Step: 2320... Loss: 1.9629... Val Loss: 1.9110\n",
            "Epoch: 28/55... Step: 2330... Loss: 1.9072... Val Loss: 1.9135\n",
            "Epoch: 28/55... Step: 2340... Loss: 1.9120... Val Loss: 1.9108\n",
            "Epoch: 28/55... Step: 2350... Loss: 1.9429... Val Loss: 1.9074\n",
            "Epoch: 29/55... Step: 2360... Loss: 1.9709... Val Loss: 1.9115\n",
            "Epoch: 29/55... Step: 2370... Loss: 1.9922... Val Loss: 1.9084\n",
            "Epoch: 29/55... Step: 2380... Loss: 1.8944... Val Loss: 1.9061\n",
            "Epoch: 29/55... Step: 2390... Loss: 1.9904... Val Loss: 1.9119\n",
            "Epoch: 29/55... Step: 2400... Loss: 1.9697... Val Loss: 1.9017\n",
            "Epoch: 29/55... Step: 2410... Loss: 1.9719... Val Loss: 1.9045\n",
            "Epoch: 29/55... Step: 2420... Loss: 2.0024... Val Loss: 1.9019\n",
            "Epoch: 29/55... Step: 2430... Loss: 1.9859... Val Loss: 1.8977\n",
            "Epoch: 30/55... Step: 2440... Loss: 1.8951... Val Loss: 1.8980\n",
            "Epoch: 30/55... Step: 2450... Loss: 1.8477... Val Loss: 1.9037\n",
            "Epoch: 30/55... Step: 2460... Loss: 1.8938... Val Loss: 1.8971\n",
            "Epoch: 30/55... Step: 2470... Loss: 1.9114... Val Loss: 1.9012\n",
            "Epoch: 30/55... Step: 2480... Loss: 1.9376... Val Loss: 1.8940\n",
            "Epoch: 30/55... Step: 2490... Loss: 1.9701... Val Loss: 1.8958\n",
            "Epoch: 30/55... Step: 2500... Loss: 1.9813... Val Loss: 1.8956\n",
            "Epoch: 30/55... Step: 2510... Loss: 1.9880... Val Loss: 1.8886\n",
            "Epoch: 30/55... Step: 2520... Loss: 1.9617... Val Loss: 1.8899\n",
            "Epoch: 31/55... Step: 2530... Loss: 1.9674... Val Loss: 1.8915\n",
            "Epoch: 31/55... Step: 2540... Loss: 2.0164... Val Loss: 1.8855\n",
            "Epoch: 31/55... Step: 2550... Loss: 1.9075... Val Loss: 1.8865\n",
            "Epoch: 31/55... Step: 2560... Loss: 1.9237... Val Loss: 1.8898\n",
            "Epoch: 31/55... Step: 2570... Loss: 1.9766... Val Loss: 1.8830\n",
            "Epoch: 31/55... Step: 2580... Loss: 1.9515... Val Loss: 1.8865\n",
            "Epoch: 31/55... Step: 2590... Loss: 1.9058... Val Loss: 1.8919\n",
            "Epoch: 31/55... Step: 2600... Loss: 1.9010... Val Loss: 1.8866\n",
            "Epoch: 32/55... Step: 2610... Loss: 1.9420... Val Loss: 1.8840\n",
            "Epoch: 32/55... Step: 2620... Loss: 1.8765... Val Loss: 1.8806\n",
            "Epoch: 32/55... Step: 2630... Loss: 1.9263... Val Loss: 1.8791\n",
            "Epoch: 32/55... Step: 2640... Loss: 1.9280... Val Loss: 1.8838\n",
            "Epoch: 32/55... Step: 2650... Loss: 1.9719... Val Loss: 1.8787\n",
            "Epoch: 32/55... Step: 2660... Loss: 1.9443... Val Loss: 1.8775\n",
            "Epoch: 32/55... Step: 2670... Loss: 1.8404... Val Loss: 1.8773\n",
            "Epoch: 32/55... Step: 2680... Loss: 1.9454... Val Loss: 1.8743\n",
            "Epoch: 33/55... Step: 2690... Loss: 1.9693... Val Loss: 1.8751\n",
            "Epoch: 33/55... Step: 2700... Loss: 1.8545... Val Loss: 1.8761\n",
            "Epoch: 33/55... Step: 2710... Loss: 1.9151... Val Loss: 1.8707\n",
            "Epoch: 33/55... Step: 2720... Loss: 1.8673... Val Loss: 1.8729\n",
            "Epoch: 33/55... Step: 2730... Loss: 1.8619... Val Loss: 1.8698\n",
            "Epoch: 33/55... Step: 2740... Loss: 1.9082... Val Loss: 1.8660\n",
            "Epoch: 33/55... Step: 2750... Loss: 1.8504... Val Loss: 1.8683\n",
            "Epoch: 33/55... Step: 2760... Loss: 1.8863... Val Loss: 1.8702\n",
            "Epoch: 33/55... Step: 2770... Loss: 1.8664... Val Loss: 1.8689\n",
            "Epoch: 34/55... Step: 2780... Loss: 1.9007... Val Loss: 1.8666\n",
            "Epoch: 34/55... Step: 2790... Loss: 1.9067... Val Loss: 1.8641\n",
            "Epoch: 34/55... Step: 2800... Loss: 1.8435... Val Loss: 1.8686\n",
            "Epoch: 34/55... Step: 2810... Loss: 1.9273... Val Loss: 1.8668\n",
            "Epoch: 34/55... Step: 2820... Loss: 1.9151... Val Loss: 1.8622\n",
            "Epoch: 34/55... Step: 2830... Loss: 1.8812... Val Loss: 1.8638\n",
            "Epoch: 34/55... Step: 2840... Loss: 1.9875... Val Loss: 1.8637\n",
            "Epoch: 34/55... Step: 2850... Loss: 1.9227... Val Loss: 1.8591\n",
            "Epoch: 35/55... Step: 2860... Loss: 1.8618... Val Loss: 1.8561\n",
            "Epoch: 35/55... Step: 2870... Loss: 1.8271... Val Loss: 1.8600\n",
            "Epoch: 35/55... Step: 2880... Loss: 1.8677... Val Loss: 1.8561\n",
            "Epoch: 35/55... Step: 2890... Loss: 1.8308... Val Loss: 1.8556\n",
            "Epoch: 35/55... Step: 2900... Loss: 1.9260... Val Loss: 1.8544\n",
            "Epoch: 35/55... Step: 2910... Loss: 1.9176... Val Loss: 1.8538\n",
            "Epoch: 35/55... Step: 2920... Loss: 1.9194... Val Loss: 1.8554\n",
            "Epoch: 35/55... Step: 2930... Loss: 1.9113... Val Loss: 1.8502\n",
            "Epoch: 35/55... Step: 2940... Loss: 1.9200... Val Loss: 1.8597\n",
            "Epoch: 36/55... Step: 2950... Loss: 1.9179... Val Loss: 1.8562\n",
            "Epoch: 36/55... Step: 2960... Loss: 1.9563... Val Loss: 1.8472\n",
            "Epoch: 36/55... Step: 2970... Loss: 1.8671... Val Loss: 1.8467\n",
            "Epoch: 36/55... Step: 2980... Loss: 1.8740... Val Loss: 1.8481\n",
            "Epoch: 36/55... Step: 2990... Loss: 1.8905... Val Loss: 1.8475\n",
            "Epoch: 36/55... Step: 3000... Loss: 1.8844... Val Loss: 1.8496\n",
            "Epoch: 36/55... Step: 3010... Loss: 1.8419... Val Loss: 1.8491\n",
            "Epoch: 36/55... Step: 3020... Loss: 1.8826... Val Loss: 1.8499\n",
            "Epoch: 37/55... Step: 3030... Loss: 1.8932... Val Loss: 1.8471\n",
            "Epoch: 37/55... Step: 3040... Loss: 1.8596... Val Loss: 1.8448\n",
            "Epoch: 37/55... Step: 3050... Loss: 1.8506... Val Loss: 1.8472\n",
            "Epoch: 37/55... Step: 3060... Loss: 1.8917... Val Loss: 1.8450\n",
            "Epoch: 37/55... Step: 3070... Loss: 1.9284... Val Loss: 1.8416\n",
            "Epoch: 37/55... Step: 3080... Loss: 1.8794... Val Loss: 1.8403\n",
            "Epoch: 37/55... Step: 3090... Loss: 1.7756... Val Loss: 1.8437\n",
            "Epoch: 37/55... Step: 3100... Loss: 1.9039... Val Loss: 1.8406\n",
            "Epoch: 38/55... Step: 3110... Loss: 1.9074... Val Loss: 1.8412\n",
            "Epoch: 38/55... Step: 3120... Loss: 1.8499... Val Loss: 1.8405\n",
            "Epoch: 38/55... Step: 3130... Loss: 1.8526... Val Loss: 1.8354\n",
            "Epoch: 38/55... Step: 3140... Loss: 1.8312... Val Loss: 1.8428\n",
            "Epoch: 38/55... Step: 3150... Loss: 1.8223... Val Loss: 1.8390\n",
            "Epoch: 38/55... Step: 3160... Loss: 1.8497... Val Loss: 1.8350\n",
            "Epoch: 38/55... Step: 3170... Loss: 1.8162... Val Loss: 1.8363\n",
            "Epoch: 38/55... Step: 3180... Loss: 1.8138... Val Loss: 1.8351\n",
            "Epoch: 38/55... Step: 3190... Loss: 1.8065... Val Loss: 1.8311\n",
            "Epoch: 39/55... Step: 3200... Loss: 1.8587... Val Loss: 1.8327\n",
            "Epoch: 39/55... Step: 3210... Loss: 1.8471... Val Loss: 1.8329\n",
            "Epoch: 39/55... Step: 3220... Loss: 1.8022... Val Loss: 1.8345\n",
            "Epoch: 39/55... Step: 3230... Loss: 1.8586... Val Loss: 1.8329\n",
            "Epoch: 39/55... Step: 3240... Loss: 1.8687... Val Loss: 1.8263\n",
            "Epoch: 39/55... Step: 3250... Loss: 1.8526... Val Loss: 1.8302\n",
            "Epoch: 39/55... Step: 3260... Loss: 1.8899... Val Loss: 1.8307\n",
            "Epoch: 39/55... Step: 3270... Loss: 1.8739... Val Loss: 1.8307\n",
            "Epoch: 40/55... Step: 3280... Loss: 1.8373... Val Loss: 1.8228\n",
            "Epoch: 40/55... Step: 3290... Loss: 1.7830... Val Loss: 1.8288\n",
            "Epoch: 40/55... Step: 3300... Loss: 1.8064... Val Loss: 1.8268\n",
            "Epoch: 40/55... Step: 3310... Loss: 1.7917... Val Loss: 1.8267\n",
            "Epoch: 40/55... Step: 3320... Loss: 1.8103... Val Loss: 1.8242\n",
            "Epoch: 40/55... Step: 3330... Loss: 1.8628... Val Loss: 1.8221\n",
            "Epoch: 40/55... Step: 3340... Loss: 1.8913... Val Loss: 1.8247\n",
            "Epoch: 40/55... Step: 3350... Loss: 1.8671... Val Loss: 1.8188\n",
            "Epoch: 40/55... Step: 3360... Loss: 1.8674... Val Loss: 1.8234\n",
            "Epoch: 41/55... Step: 3370... Loss: 1.8516... Val Loss: 1.8255\n",
            "Epoch: 41/55... Step: 3380... Loss: 1.8910... Val Loss: 1.8187\n",
            "Epoch: 41/55... Step: 3390... Loss: 1.8274... Val Loss: 1.8222\n",
            "Epoch: 41/55... Step: 3400... Loss: 1.8020... Val Loss: 1.8155\n",
            "Epoch: 41/55... Step: 3410... Loss: 1.8249... Val Loss: 1.8226\n",
            "Epoch: 41/55... Step: 3420... Loss: 1.8377... Val Loss: 1.8200\n",
            "Epoch: 41/55... Step: 3430... Loss: 1.8230... Val Loss: 1.8228\n",
            "Epoch: 41/55... Step: 3440... Loss: 1.8481... Val Loss: 1.8196\n",
            "Epoch: 42/55... Step: 3450... Loss: 1.8549... Val Loss: 1.8170\n",
            "Epoch: 42/55... Step: 3460... Loss: 1.8129... Val Loss: 1.8162\n",
            "Epoch: 42/55... Step: 3470... Loss: 1.8438... Val Loss: 1.8204\n",
            "Epoch: 42/55... Step: 3480... Loss: 1.8455... Val Loss: 1.8168\n",
            "Epoch: 42/55... Step: 3490... Loss: 1.8430... Val Loss: 1.8138\n",
            "Epoch: 42/55... Step: 3500... Loss: 1.8365... Val Loss: 1.8129\n",
            "Epoch: 42/55... Step: 3510... Loss: 1.7385... Val Loss: 1.8212\n",
            "Epoch: 42/55... Step: 3520... Loss: 1.8332... Val Loss: 1.8133\n",
            "Epoch: 43/55... Step: 3530... Loss: 1.9029... Val Loss: 1.8143\n",
            "Epoch: 43/55... Step: 3540... Loss: 1.7931... Val Loss: 1.8155\n",
            "Epoch: 43/55... Step: 3550... Loss: 1.8206... Val Loss: 1.8095\n",
            "Epoch: 43/55... Step: 3560... Loss: 1.7871... Val Loss: 1.8148\n",
            "Epoch: 43/55... Step: 3570... Loss: 1.8038... Val Loss: 1.8096\n",
            "Epoch: 43/55... Step: 3580... Loss: 1.8736... Val Loss: 1.8193\n",
            "Epoch: 43/55... Step: 3590... Loss: 1.7619... Val Loss: 1.8095\n",
            "Epoch: 43/55... Step: 3600... Loss: 1.7780... Val Loss: 1.8095\n",
            "Epoch: 43/55... Step: 3610... Loss: 1.7880... Val Loss: 1.8127\n",
            "Epoch: 44/55... Step: 3620... Loss: 1.8279... Val Loss: 1.8087\n",
            "Epoch: 44/55... Step: 3630... Loss: 1.8234... Val Loss: 1.8115\n",
            "Epoch: 44/55... Step: 3640... Loss: 1.7903... Val Loss: 1.8129\n",
            "Epoch: 44/55... Step: 3650... Loss: 1.8491... Val Loss: 1.8095\n",
            "Epoch: 44/55... Step: 3660... Loss: 1.8465... Val Loss: 1.8015\n",
            "Epoch: 44/55... Step: 3670... Loss: 1.8199... Val Loss: 1.8051\n",
            "Epoch: 44/55... Step: 3680... Loss: 1.8801... Val Loss: 1.8039\n",
            "Epoch: 44/55... Step: 3690... Loss: 1.8131... Val Loss: 1.8033\n",
            "Epoch: 45/55... Step: 3700... Loss: 1.8071... Val Loss: 1.7997\n",
            "Epoch: 45/55... Step: 3710... Loss: 1.7558... Val Loss: 1.8065\n",
            "Epoch: 45/55... Step: 3720... Loss: 1.7978... Val Loss: 1.8019\n",
            "Epoch: 45/55... Step: 3730... Loss: 1.7623... Val Loss: 1.8066\n",
            "Epoch: 45/55... Step: 3740... Loss: 1.8139... Val Loss: 1.8024\n",
            "Epoch: 45/55... Step: 3750... Loss: 1.8144... Val Loss: 1.7987\n",
            "Epoch: 45/55... Step: 3760... Loss: 1.8572... Val Loss: 1.8019\n",
            "Epoch: 45/55... Step: 3770... Loss: 1.8713... Val Loss: 1.7991\n",
            "Epoch: 45/55... Step: 3780... Loss: 1.8243... Val Loss: 1.8096\n",
            "Epoch: 46/55... Step: 3790... Loss: 1.8165... Val Loss: 1.8006\n",
            "Epoch: 46/55... Step: 3800... Loss: 1.8532... Val Loss: 1.7943\n",
            "Epoch: 46/55... Step: 3810... Loss: 1.7939... Val Loss: 1.7967\n",
            "Epoch: 46/55... Step: 3820... Loss: 1.7892... Val Loss: 1.7910\n",
            "Epoch: 46/55... Step: 3830... Loss: 1.8036... Val Loss: 1.7927\n",
            "Epoch: 46/55... Step: 3840... Loss: 1.7994... Val Loss: 1.7963\n",
            "Epoch: 46/55... Step: 3850... Loss: 1.7780... Val Loss: 1.7990\n",
            "Epoch: 46/55... Step: 3860... Loss: 1.7990... Val Loss: 1.7936\n",
            "Epoch: 47/55... Step: 3870... Loss: 1.8249... Val Loss: 1.7973\n",
            "Epoch: 47/55... Step: 3880... Loss: 1.7685... Val Loss: 1.8006\n",
            "Epoch: 47/55... Step: 3890... Loss: 1.8137... Val Loss: 1.7954\n",
            "Epoch: 47/55... Step: 3900... Loss: 1.8347... Val Loss: 1.7955\n",
            "Epoch: 47/55... Step: 3910... Loss: 1.8365... Val Loss: 1.7925\n",
            "Epoch: 47/55... Step: 3920... Loss: 1.8335... Val Loss: 1.7917\n",
            "Epoch: 47/55... Step: 3930... Loss: 1.7019... Val Loss: 1.7929\n",
            "Epoch: 47/55... Step: 3940... Loss: 1.8237... Val Loss: 1.7950\n",
            "Epoch: 48/55... Step: 3950... Loss: 1.8571... Val Loss: 1.7926\n",
            "Epoch: 48/55... Step: 3960... Loss: 1.7327... Val Loss: 1.7900\n",
            "Epoch: 48/55... Step: 3970... Loss: 1.7752... Val Loss: 1.7888\n",
            "Epoch: 48/55... Step: 3980... Loss: 1.7511... Val Loss: 1.7936\n",
            "Epoch: 48/55... Step: 3990... Loss: 1.7630... Val Loss: 1.7908\n",
            "Epoch: 48/55... Step: 4000... Loss: 1.7899... Val Loss: 1.7911\n",
            "Epoch: 48/55... Step: 4010... Loss: 1.7182... Val Loss: 1.7895\n",
            "Epoch: 48/55... Step: 4020... Loss: 1.7649... Val Loss: 1.7894\n",
            "Epoch: 48/55... Step: 4030... Loss: 1.7600... Val Loss: 1.7904\n",
            "Epoch: 49/55... Step: 4040... Loss: 1.8110... Val Loss: 1.7864\n",
            "Epoch: 49/55... Step: 4050... Loss: 1.7995... Val Loss: 1.7904\n",
            "Epoch: 49/55... Step: 4060... Loss: 1.7200... Val Loss: 1.7907\n",
            "Epoch: 49/55... Step: 4070... Loss: 1.8269... Val Loss: 1.7881\n",
            "Epoch: 49/55... Step: 4080... Loss: 1.8167... Val Loss: 1.7800\n",
            "Epoch: 49/55... Step: 4090... Loss: 1.7943... Val Loss: 1.7841\n",
            "Epoch: 49/55... Step: 4100... Loss: 1.8250... Val Loss: 1.7922\n",
            "Epoch: 49/55... Step: 4110... Loss: 1.8372... Val Loss: 1.7882\n",
            "Epoch: 50/55... Step: 4120... Loss: 1.7558... Val Loss: 1.7803\n",
            "Epoch: 50/55... Step: 4130... Loss: 1.7019... Val Loss: 1.7909\n",
            "Epoch: 50/55... Step: 4140... Loss: 1.7623... Val Loss: 1.7842\n",
            "Epoch: 50/55... Step: 4150... Loss: 1.7204... Val Loss: 1.7870\n",
            "Epoch: 50/55... Step: 4160... Loss: 1.7731... Val Loss: 1.7837\n",
            "Epoch: 50/55... Step: 4170... Loss: 1.7878... Val Loss: 1.7815\n",
            "Epoch: 50/55... Step: 4180... Loss: 1.8053... Val Loss: 1.7849\n",
            "Epoch: 50/55... Step: 4190... Loss: 1.8162... Val Loss: 1.7802\n",
            "Epoch: 50/55... Step: 4200... Loss: 1.7940... Val Loss: 1.7883\n",
            "Epoch: 51/55... Step: 4210... Loss: 1.7957... Val Loss: 1.7836\n",
            "Epoch: 51/55... Step: 4220... Loss: 1.8503... Val Loss: 1.7784\n",
            "Epoch: 51/55... Step: 4230... Loss: 1.7774... Val Loss: 1.7814\n",
            "Epoch: 51/55... Step: 4240... Loss: 1.7516... Val Loss: 1.7743\n",
            "Epoch: 51/55... Step: 4250... Loss: 1.7907... Val Loss: 1.7810\n",
            "Epoch: 51/55... Step: 4260... Loss: 1.7736... Val Loss: 1.7770\n",
            "Epoch: 51/55... Step: 4270... Loss: 1.7143... Val Loss: 1.7838\n",
            "Epoch: 51/55... Step: 4280... Loss: 1.7823... Val Loss: 1.7787\n",
            "Epoch: 52/55... Step: 4290... Loss: 1.7722... Val Loss: 1.7787\n",
            "Epoch: 52/55... Step: 4300... Loss: 1.7187... Val Loss: 1.7861\n",
            "Epoch: 52/55... Step: 4310... Loss: 1.7885... Val Loss: 1.7833\n",
            "Epoch: 52/55... Step: 4320... Loss: 1.7878... Val Loss: 1.7825\n",
            "Epoch: 52/55... Step: 4330... Loss: 1.8108... Val Loss: 1.7756\n",
            "Epoch: 52/55... Step: 4340... Loss: 1.7566... Val Loss: 1.7798\n",
            "Epoch: 52/55... Step: 4350... Loss: 1.6815... Val Loss: 1.7761\n",
            "Epoch: 52/55... Step: 4360... Loss: 1.7756... Val Loss: 1.7759\n",
            "Epoch: 53/55... Step: 4370... Loss: 1.8251... Val Loss: 1.7806\n",
            "Epoch: 53/55... Step: 4380... Loss: 1.7105... Val Loss: 1.7780\n",
            "Epoch: 53/55... Step: 4390... Loss: 1.7498... Val Loss: 1.7766\n",
            "Epoch: 53/55... Step: 4400... Loss: 1.7313... Val Loss: 1.7835\n",
            "Epoch: 53/55... Step: 4410... Loss: 1.7035... Val Loss: 1.7738\n",
            "Epoch: 53/55... Step: 4420... Loss: 1.7807... Val Loss: 1.7720\n",
            "Epoch: 53/55... Step: 4430... Loss: 1.6848... Val Loss: 1.7731\n",
            "Epoch: 53/55... Step: 4440... Loss: 1.7502... Val Loss: 1.7745\n",
            "Epoch: 53/55... Step: 4450... Loss: 1.7218... Val Loss: 1.7772\n",
            "Epoch: 54/55... Step: 4460... Loss: 1.7803... Val Loss: 1.7733\n",
            "Epoch: 54/55... Step: 4470... Loss: 1.7309... Val Loss: 1.7728\n",
            "Epoch: 54/55... Step: 4480... Loss: 1.6958... Val Loss: 1.7715\n",
            "Epoch: 54/55... Step: 4490... Loss: 1.7924... Val Loss: 1.7723\n",
            "Epoch: 54/55... Step: 4500... Loss: 1.7667... Val Loss: 1.7661\n",
            "Epoch: 54/55... Step: 4510... Loss: 1.7980... Val Loss: 1.7714\n",
            "Epoch: 54/55... Step: 4520... Loss: 1.8025... Val Loss: 1.7731\n",
            "Epoch: 54/55... Step: 4530... Loss: 1.7638... Val Loss: 1.7717\n",
            "Epoch: 55/55... Step: 4540... Loss: 1.7208... Val Loss: 1.7686\n",
            "Epoch: 55/55... Step: 4550... Loss: 1.7072... Val Loss: 1.7791\n",
            "Epoch: 55/55... Step: 4560... Loss: 1.7409... Val Loss: 1.7674\n",
            "Epoch: 55/55... Step: 4570... Loss: 1.6814... Val Loss: 1.7657\n",
            "Epoch: 55/55... Step: 4580... Loss: 1.7499... Val Loss: 1.7694\n",
            "Epoch: 55/55... Step: 4590... Loss: 1.7951... Val Loss: 1.7668\n",
            "Epoch: 55/55... Step: 4600... Loss: 1.7938... Val Loss: 1.7708\n",
            "Epoch: 55/55... Step: 4610... Loss: 1.8018... Val Loss: 1.7711\n",
            "Epoch: 55/55... Step: 4620... Loss: 1.7605... Val Loss: 1.7719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ylMWIpPT2n08",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Analysing and Testing</h2>"
      ]
    },
    {
      "metadata": {
        "id": "27xTn1H5OgNz",
        "colab_type": "code",
        "outputId": "258e5332-3484-40d9-c8a0-a96e0cc36cd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#analysising training and validation loss over the training time\n",
        "plt.plot(train_losses)\n",
        "plt.plot(val_losses)\n",
        "plt.legend([\"Training Loss\",\"Validation Loss\"])\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Unmk1mUByYf3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is clear from the graph that if the model is trained any further it will overfit the training data. Even after trying with all possible hyperparameters this was the best result. This means that 94k characters aren't sufficient to train our model. Introducing more characters, around 10mil, will allow us to increase the size of our hidden layer which will increase the accuracy of our model."
      ]
    },
    {
      "metadata": {
        "id": "08tVse778u5Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#saving the model\n",
        "model_name = 'lstm_55_epoch_128_h_50_dp.net'\n",
        "\n",
        "checkpoint = {'n_hidden': model.n_hidden,\n",
        "              'n_layers': model.n_layers,\n",
        "              'state_dict': model.state_dict(),\n",
        "              'chars_len': model.chars_len}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0e5oLejURYrx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def predict(model, char, h=None, top_k=None):\n",
        "  ''' method to predict next character in the sequence\n",
        "  \n",
        "  model: model to be used to predict character\n",
        "  char: int of current character, i.e input for the model\n",
        "  h: hidden layer \n",
        "  top_k: no. of top k possible characters\n",
        "  '''\n",
        "\n",
        "  # tensor inputs\n",
        "  x = np.array([[char_2_int[char]]])\n",
        "  x = one_hot_encoding(x, model.chars_len)\n",
        "  inputs = torch.from_numpy(x).to(device)\n",
        "\n",
        "  # detach hidden state from history\n",
        "  h = tuple([each.data for each in h])\n",
        "  # get the output of the model\n",
        "  out, h = model(inputs, h)\n",
        "\n",
        "  # get the character probabilities\n",
        "  p = F.softmax(out, dim=1).data\n",
        "\n",
        "  p = p.cpu() # move to cpu\n",
        "\n",
        "  # get top characters\n",
        "  if top_k is None:\n",
        "      top_ch = np.arange(len(model.chars))\n",
        "  else:\n",
        "      p, top_ch = p.topk(top_k)\n",
        "      top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "  # select the likely next character with some element of randomness\n",
        "  p = p.numpy().squeeze()\n",
        "  char = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return int_2_char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VssUyVmLoPvp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(model, size, prime='The', top_k=None):\n",
        "  \"\"\"method to sample or generate string fraom predicted characters\n",
        "  \n",
        "  model: model used to predict\n",
        "  size: number of characters to be predicted\n",
        "  prime: intialising sequence\n",
        "  top_k: no. of top k possible characters\"\"\"\n",
        "  \n",
        "  model = model.to(device)\n",
        "  model.eval() # eval mode\n",
        "\n",
        "  # First off, run through the prime characters\n",
        "  chars = [ch for ch in prime]\n",
        "  h = model.init_hidden(1,device)\n",
        "  for ch in prime:\n",
        "      char, h = predict(model, ch, h, top_k=top_k)\n",
        "\n",
        "  chars.append(char)\n",
        "\n",
        "  # Now pass in the previous character and get a new one\n",
        "  for ii in range(size):\n",
        "      char, h = predict(model, chars[-1], h, top_k=top_k)\n",
        "      chars.append(char)\n",
        "\n",
        "  return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F0WjoAZKoWmy",
        "colab_type": "code",
        "outputId": "dfa0f7f7-e06c-4c98-e872-68e7062c9e50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "#testing the model\n",
        "print(sample(model, 200, prime='From', top_k=2))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From thy strengle she to the to the she shate she thee.\n",
            "And then to my love the songre and the strom,\n",
            "And then to my line the strengle that the world,\n",
            "Then thou shall the some then the store,\n",
            "And then so t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i3o_TG6u2zT6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "While there are meaningfull words in the output, there is substancial lack of structure in the content. This can be improved if the model is trained for longer time on a bigger data. Andrew Karpathy mentions in his post that he trained his model for an hour on Shakespeare's dramas to achieve the result mentioned in his post. <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">Andrew Karpathy's post</a>"
      ]
    },
    {
      "metadata": {
        "id": "iPtzyXRsochW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}